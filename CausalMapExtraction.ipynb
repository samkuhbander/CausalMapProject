{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-m29IoRYR7T"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk21Kv7GYViL"
      },
      "source": [
        "The goal of this program is to perform semantic search and causal map extraction from PDF documents efficiently and effectively. In particular, we focus on identifying sentences that support and align with the causal relationships depicted in causal maps. Causal maps are graphical representations that illustrate the cause-and-effect relationships between variables in a system.\n",
        "\n",
        "Our program consists of several key steps, each designed to save time and ensure quality results:\n",
        "\n",
        "Preprocessing PDF Documents: We start by converting the PDF documents into a machine-readable format. During this step, we extract textual content and perform essential preprocessing, such as removing any content that is not useful for analysis (e.g., citations, references).\n",
        "\n",
        "Embedding and Similarity Search: The extracted sentences are then processed using OpenAI's embeddings to create vector representations of the text. By applying cosine similarity, we compare the embeddings of the sentences to the embeddings of queries derived from the causal maps. This process enables us to identify sentences that closely match the causal relationships described in the maps.\n",
        "\n",
        "Extracting Causal Maps: In addition to identifying supportive sentences, we also extract causal maps from sentences that describe multiple effects of a given cause. This extraction process is crucial for preventing hallucinations—spurious or ungrounded information—in machine learning models trained on the data. Sentences with multiple effects provide a detailed view of the causal system, allowing models to better understand the complexity of real-world causality.\n",
        "\n",
        "Analysis and Visualization: We analyze and visualize the distribution of similarity scores obtained from the semantic search process. This step provides valuable insights into the quality of the semantic search results and helps us determine an appropriate cutoff score for filtering sentences.\n",
        "\n",
        "Overall, this program provides a comprehensive and efficient approach to enhancing the quality of causal data used in machine learning models. By identifying sentences that support causal maps, extracting detailed causal information, and streamlining the analysis process, we improve the interpretability and reliability of insights derived from the models while saving time in the overall workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypW8ATzDy1e2"
      },
      "source": [
        "# Setup and Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsonwirAbCZg"
      },
      "source": [
        "Before proceeding with the main steps, we install the necessary Python libraries and dependencies required for this project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz8rWVGDmCMD",
        "outputId": "e5eaf803-1342-4f4d-e9d4-047b4f5cd287"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF \n",
        "!pip install openai\n",
        "\n",
        "# Importing necessary libraries\n",
        "# Built-in Python libraries, no versions required\n",
        "import csv  \n",
        "import json  \n",
        "import os \n",
        "\n",
        "import networkx as nx  # networkx==3.0\n",
        "import matplotlib.pyplot as plt  # matplotlib==3.5.1\n",
        "import nltk  # nltk==3.8.1\n",
        "nltk.download('punkt')\n",
        "import pandas as pd  # pandas==1.5.3 \n",
        "import openai  # openai==0.26.5\n",
        "import numpy as np  # numpy==1.24.1\n",
        "from operator import itemgetter  \n",
        "from matplotlib import patches  # matplotlib==3.5.1\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "# Importing PyMuPDF, aliased as fitz\n",
        "import fitz  # PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCNFOxXdEpc"
      },
      "source": [
        "Next we add an OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4T_-jPari0p"
      },
      "outputs": [],
      "source": [
        "# Replace \"YOUR_API_KEY_HERE\" with your actual OpenAI API key\n",
        "openai.api_key = \"\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVn6xzgjzpJY"
      },
      "outputs": [],
      "source": [
        "directories_to_create = [\"pdf_files\", \"json_files\", \"csv_files\", \"embedded_files\", \"maps\", \"results\"]\n",
        "#Check if the directories exist in causalmapcapstone folder\n",
        "def create_directories(dir_list):\n",
        "    for directory in dir_list:\n",
        "        if not os.path.exists(directory):\n",
        "            os.mkdir(directory)\n",
        "create_directories(directories_to_create)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMWsTNav0omA"
      },
      "source": [
        "# File Conversion and Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCzYNzbgakML"
      },
      "source": [
        "In this step, we extract and clean the textual content from PDF documents for further analysis:\n",
        "\n",
        "Font Information: We identify different font sizes in the document to distinguish between headers, paragraphs, and other text elements.\n",
        "\n",
        "Extracting Text: We extract text, skipping sections like \"works cited\" or \"bibliography\" that don't contain valuable information. \n",
        "\n",
        "Cleaning Text: We remove special characters, extra whitespaces, and unwanted sentences (e.g., with URLs) from the text. We also tokenize the text into individual sentences.\n",
        "\n",
        "Saving Results: The clean sentences are saved in a structured format for further use.\n",
        "\n",
        "As a result, we obtain a list of clean sentences from the PDF document, ready for the next steps of analysis, including generating embeddings and similarity searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cenFWOMBs_EY"
      },
      "outputs": [],
      "source": [
        "def fonts(doc, granularity=False):\n",
        "    \"\"\"Extracts fonts and their usage in PDF documents.\n",
        "    :param doc: PDF document to iterate through\n",
        "    :type doc: <class 'fitz.fitz.Document'>\n",
        "    :param granularity: also use 'font', 'flags' and 'color' to discriminate text\n",
        "    :type granularity: bool\n",
        "    :rtype: [(font_size, count), (font_size, count}], dict\n",
        "    :return: most used fonts sorted by count, font style information\n",
        "    \"\"\"\n",
        "    styles = {}\n",
        "    font_counts = {}\n",
        "\n",
        "    for page in doc:\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        for b in blocks:  # iterate through the text blocks\n",
        "            if b['type'] == 0:  # block contains text\n",
        "                for l in b[\"lines\"]:  # iterate through the text lines\n",
        "                    for s in l[\"spans\"]:  # iterate through the text spans\n",
        "                        if granularity:\n",
        "                            identifier = \"{0}_{1}_{2}_{3}\".format(s['size'], s['flags'], s['font'], s['color'])\n",
        "                            styles[identifier] = {'size': s['size'], 'flags': s['flags'], 'font': s['font'],\n",
        "                                                  'color': s['color']}\n",
        "                        else:\n",
        "                            identifier = \"{0}\".format(s['size'])\n",
        "                            styles[identifier] = {'size': s['size'], 'font': s['font']}\n",
        "\n",
        "                        font_counts[identifier] = font_counts.get(identifier, 0) + 1  # count the fonts usage\n",
        "\n",
        "    font_counts = sorted(font_counts.items(), key=itemgetter(1), reverse=True)\n",
        "\n",
        "    if len(font_counts) < 1:\n",
        "        raise ValueError(\"Zero discriminating fonts found!\")\n",
        "\n",
        "    return font_counts, styles\n",
        "\n",
        "\n",
        "def font_tags(font_counts, styles):\n",
        "    \"\"\"Returns dictionary with font sizes as keys and tags as value.\n",
        "    :param font_counts: (font_size, count) for all fonts occuring in document\n",
        "    :type font_counts: list\n",
        "    :param styles: all styles found in the document\n",
        "    :type styles: dict\n",
        "    :rtype: dict\n",
        "    :return: all element tags based on font-sizes\n",
        "    \"\"\"\n",
        "    p_style = styles[font_counts[0][0]]  # get style for most used font by count (paragraph)\n",
        "    p_size = p_style['size']  # get the paragraph's size\n",
        "\n",
        "    # sorting the font sizes high to low, so that we can append the right integer to each tag\n",
        "    font_sizes = []\n",
        "    for (font_size, count) in font_counts:\n",
        "        font_sizes.append(float(font_size))\n",
        "    font_sizes.sort(reverse=True)\n",
        "\n",
        "    # aggregating the tags for each font size\n",
        "    idx = 0\n",
        "    size_tag = {}\n",
        "    for size in font_sizes:\n",
        "        idx += 1\n",
        "        if size == p_size:\n",
        "            idx = 0\n",
        "            size_tag[size] = '<p>'\n",
        "        if size > p_size:\n",
        "            size_tag[size] = '<h{0}>'.format(idx)\n",
        "        elif size < p_size:\n",
        "            size_tag[size] = '<s{0}>'.format(idx)\n",
        "\n",
        "    return size_tag\n",
        "\n",
        "\n",
        "def block_ended(current_size, new_size, new_text, skip_words):\n",
        "    return new_size != current_size or any(word in new_text.lower() for word in skip_words)\n",
        "\n",
        "def headers_para(doc, size_tag):\n",
        "    #Sections that match these words will be removed\n",
        "    skip_words = ['works cited', 'bibliography', 'reference', 'citation']\n",
        "    header_para = []\n",
        "    current_size = None\n",
        "    block_string = \"\"\n",
        "    skip_section = False\n",
        "    skip_section_size = 0\n",
        "\n",
        "    for page in doc:\n",
        "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "        for b in blocks:\n",
        "            if b['type'] == 0:\n",
        "                for l in b[\"lines\"]:\n",
        "                    for s in l[\"spans\"]:\n",
        "                        text = s['text'].strip()\n",
        "                        if not text:\n",
        "                            continue\n",
        "\n",
        "                        # Skip block if it only contains pipes\n",
        "                        if all((c == \"|\") for c in text):\n",
        "                            continue\n",
        "\n",
        "                        text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "                        text = \" \".join(text.split())  # Replace multiple whitespaces with single whitespace\n",
        "                        text_size = s['size']\n",
        "                        \n",
        "                        if skip_section:\n",
        "                            if text_size > skip_section_size:\n",
        "                                skip_section = False\n",
        "                                skip_section_size = 0\n",
        "                            continue\n",
        "                        elif any(word in text.lower() for word in skip_words):\n",
        "                            skip_section = True\n",
        "                            skip_section_size = text_size\n",
        "                            continue\n",
        "                        \n",
        "                        if block_ended(current_size, text_size, text, skip_words):\n",
        "                            if block_string:\n",
        "                                header_para.append(block_string.strip())\n",
        "                            block_string = text\n",
        "                            current_size = text_size\n",
        "                        else:\n",
        "                            block_string += \" \" + text\n",
        "\n",
        "    if block_string:\n",
        "        header_para.append(block_string.strip())\n",
        "\n",
        "    return header_para"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omhTUBAht8ev",
        "outputId": "a01359c9-f36d-49e7-a267-22c587264171"
      },
      "outputs": [],
      "source": [
        "def process_files():\n",
        "    \"\"\"Converts PDF files in 'pdf_files' directory to JSON files in 'json_files' directory.\n",
        "    Each sentence is split into a new line using tokenization. Unwanted sentences are filtered out.\"\"\"\n",
        "    pdf_dir = 'pdf_files'\n",
        "    json_dir = 'json_files'\n",
        "    \n",
        "    print(\"Converting pdfs to json\")\n",
        "    for filename in os.listdir(pdf_dir):\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(pdf_dir, filename)\n",
        "            with fitz.open(pdf_path) as doc:\n",
        "                font_counts, styles = fonts(doc, granularity=False)\n",
        "                size_tag = font_tags(font_counts, styles)\n",
        "                elements = headers_para(doc, size_tag)\n",
        "\n",
        "                # Split each sentence into a new line using tokenization\n",
        "                elements = [nltk.sent_tokenize(element) for element in elements]\n",
        "\n",
        "                # Flatten the list\n",
        "                elements = [item for sublist in elements for item in sublist]\n",
        "\n",
        "                # Filter conditions for unwanted sentences that remain\n",
        "                remove_list = ['http', 'www.', '.com', '.org', '.edu', '.pdf', '....']\n",
        "                elements = [element for element in elements if len(element) > 50 and not any(word in element.lower() for word in remove_list)]\n",
        "\n",
        "                # Name the json file the same as the pdf file\n",
        "                json_filename = filename.replace('.pdf', '.json')\n",
        "                json_path = os.path.join(json_dir, json_filename)\n",
        "\n",
        "                # Remove duplicate elements\n",
        "                elements = list(dict.fromkeys(elements))\n",
        "\n",
        "                # Write the elements to a JSON file\n",
        "                with open(json_path, 'w') as f:\n",
        "                    # Format the json file\n",
        "                    json.dump(elements, f, indent=4)\n",
        "          \n",
        "process_files()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRYaitoV022Q"
      },
      "source": [
        "# Text Embedding and Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yO8_d-4bfsF"
      },
      "source": [
        "In this step, we utilize the OpenAI embeddings to convert the text data extracted from the PDF documents into continuous vector representations, commonly known as embeddings. These embeddings capture the semantic meaning of the text and are crucial for computing similarity between sentences and queries. By applying OpenAI's get_embedding function, we obtain the embeddings for each sentence, and store them in a DataFrame for later use in the semantic search process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhTIm9Y4ubwz"
      },
      "outputs": [],
      "source": [
        "# function to convert json files to csv\n",
        "def json_to_csv(filename: str) -> None:\n",
        "    df = pd.read_json(f\"json_files/{filename}\")\n",
        "    df = df.rename(columns={0: \"text\"})\n",
        "    csv_filename = filename.replace(\".json\", \".csv\")\n",
        "    df.to_csv(f\"csv_files/{csv_filename}\", index=False)\n",
        "\n",
        "# function to embed text data in a csv file using OpenAI embeddings\n",
        "def embed_file(filename: str) -> None:\n",
        "    df = pd.read_csv(f\"csv_files/{filename}\")\n",
        "    df['embedding'] = df['text'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
        "    embed_filename = filename.replace(\".csv\", \"_embed.csv\")\n",
        "    df.to_csv(f\"embedded_files/{embed_filename}\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NITpw8oO0MHp",
        "outputId": "6667f3e7-f9e6-40d4-dc10-41b00bec89af"
      },
      "outputs": [],
      "source": [
        "# Convert the json files to csv files\n",
        "for filename in os.listdir(\"json_files\"):\n",
        "    try:\n",
        "      json_to_csv(filename) \n",
        "    except:\n",
        "      print(\"\")\n",
        "    \n",
        "# Check if the file has already been embedded; otherwise, embed it\n",
        "for filename in os.listdir(\"csv_files\"):\n",
        "    if not os.path.exists(f\"embedded_files/{filename.replace('.csv', '_embed.csv')}\"):\n",
        "        print(f\"Embedding {filename}\")\n",
        "        embed_file(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvKA7PE-1aqs"
      },
      "source": [
        "# Analysis and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejj_H4C1bw2K"
      },
      "source": [
        "In this step, we analyze and visualize the distribution of similarity scores obtained from the semantic search process. We plot a histogram to depict the frequency of similarity scores within specified ranges. The visualization provides valuable insights into how closely the sentences in the document align with the queries derived from the causal maps. It helps identify the most relevant sentences that support the causal relationships and provides a way to assess the overall quality of the semantic search results. Additionally, the visualization can be used to determine an appropriate cutoff score for filtering sentences, ensuring that only those with similarity scores above the chosen threshold are retained for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSGVZAt-u_Bx"
      },
      "outputs": [],
      "source": [
        "def search_embedded_file_from_map(embedded_filename: str, map_filename: str) -> None:\n",
        "    min_results = 30  # Minimum number of total results required\n",
        "    similarity_threshold = 0.88  # Initial similarity threshold\n",
        "    similarity_step = 0.01  # Step size for decreasing similarity threshold\n",
        "    min_similarity_threshold = 0.7  # Minimum similarity threshold allowed\n",
        "\n",
        "    result_df = pd.DataFrame()\n",
        "    df = pd.read_csv(f\"embedded_files/{embedded_filename}\")\n",
        "    df['embedding'] = df['embedding'].apply(lambda x: np.array(eval(x)))\n",
        "    map_df = pd.read_csv(f\"maps/{map_filename}\")\n",
        "\n",
        "    result_filename = f\"{embedded_filename.replace('_embed.csv', '')}_{map_filename}\"\n",
        "    res = []\n",
        "\n",
        "    # Function to process an individual edge and get filtered results\n",
        "    def process_edge(query: str) -> pd.DataFrame:\n",
        "        # get the embedding for the query and calculate cosine similarity\n",
        "        query_embedding = get_embedding(query, engine='text-embedding-ada-002')\n",
        "        df['query'] = query\n",
        "        df['similarity'] = df['embedding'].apply(lambda x: cosine_similarity(x, query_embedding))\n",
        "        df_filtered = df[df['similarity'] > similarity_threshold][['query', 'text', 'similarity']].copy()\n",
        "        df_filtered.loc[:, 'similarity'] = round(df_filtered['similarity'], 4)  # add similarity row\n",
        "        return df_filtered\n",
        "\n",
        "    # Iterate over the rows of the map file and process each edge\n",
        "    for index, row in map_df.iterrows():\n",
        "        if row[2] > 0:\n",
        "            query = f\"{row[0]} causes {row[1]}\"\n",
        "        elif row[2] < 0:\n",
        "            query = f\"{row[0]} prevents {row[1]}\"\n",
        "        else:\n",
        "            continue\n",
        "        res.append(process_edge(query))\n",
        "\n",
        "    # Combine the results and sort them by similarity score\n",
        "    result_df = pd.concat(res, ignore_index=True)\n",
        "    result_df = result_df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "    # Relax the similarity threshold if total results are fewer than min_results\n",
        "    while len(result_df) < min_results and similarity_threshold > min_similarity_threshold:\n",
        "        similarity_threshold -= similarity_step\n",
        "        res = []\n",
        "        for index, row in map_df.iterrows():\n",
        "            if row[2] > 0:\n",
        "                query = f\"{row[0]} causes {row[1]}\"\n",
        "            elif row[2] < 0:\n",
        "                query = f\"{row[0]} prevents {row[1]}\"\n",
        "            else:\n",
        "                continue\n",
        "            res.append(process_edge(query))\n",
        "        result_df = pd.concat(res, ignore_index=True)\n",
        "        result_df = result_df.sort_values(by='similarity', ascending=False)\n",
        "\n",
        "    # Only include the sentence and similarity rows in the output\n",
        "    result_df = result_df[['text', 'similarity']]\n",
        "    result_df.to_csv(f'results/{result_filename}', index=False)\n",
        "\n",
        "def search_all_embedded_files(map_filename: str):\n",
        "    for embedded_filename in os.listdir(\"embedded_files\"):\n",
        "        if embedded_filename.endswith(\"_embed.csv\"):\n",
        "            search_embedded_file_from_map(embedded_filename, map_filename)\n",
        "\n",
        "# Execution (make sure to provide the correct map_filename and have necessary CSV files)\n",
        "map_filename = \"Drasic et al (edges).csv\"  # modify as needed\n",
        "search_all_embedded_files(map_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "O43tmZLSgBov",
        "outputId": "7fdc3360-9047-44ce-d837-17488070971c"
      },
      "outputs": [],
      "source": [
        "# function to get distribution of similarity scores from a result file\n",
        "def get_distribution(filename: str, sim_range: tuple) -> None:\n",
        "    df = pd.read_csv(f\"results/{filename}\")\n",
        "    # plot a histogram of the similarity scores within the specified range\n",
        "    plt.hist(df['similarity'], bins=30, range=sim_range)\n",
        "    plt.title(f\"Distribution of Similarity Scores of \\n{filename}\")\n",
        "    plt.xlabel(\"Similarity Score\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "    df = df[df['similarity'] > .87]\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "get_distribution(\"[2012-36] Kentucky Task Force on Childhood Obesity_Drasic et al (edges).csv\", sim_range=(.85, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "CWvkcsBQeq2X",
        "outputId": "87b8f072-9477-4f62-d994-accc874bc4b2"
      },
      "outputs": [],
      "source": [
        "def makeViolinPlot():\n",
        "    #Read in \"PDFExtractorTop20.csv\"\n",
        "    #Get the false positives, false negatives, true positives, and true negatives\n",
        "    #Make a violin plot\n",
        "    #Read in the csv file\n",
        "    with open(\"PDFExtractorTop20.csv\", 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        falsePositives86 = []\n",
        "        falsePositives87 = []\n",
        "        falsePostives88 = []\n",
        "        falsePostives89 = []\n",
        "        for row in reader:\n",
        "            if row[0] == \"0\" and float(row[3]) > .86:\n",
        "                falsePositives86.append(float(row[3]))\n",
        "            if row[0] == \"0\" and float(row[3]) > .87:\n",
        "                falsePositives87.append(float(row[3]))\n",
        "            if row[0] == \"0\" and float(row[3]) > .88:\n",
        "                falsePostives88.append(float(row[3]))\n",
        "            if row[0] == \"0\" and float(row[3]) > .89:\n",
        "                falsePostives89.append(float(row[3]))\n",
        "        #Make a violin plot\n",
        "        data = [falsePositives86, falsePositives87, falsePostives88, falsePostives89]\n",
        "        labels = [\"85%\", \"86%\", \"87%\", \"88%\", \"89%\"]\n",
        "        plt.violinplot(data, showmeans=True, showmedians=True)\n",
        "        plt.xticks([1, 2, 3, 4, 5], labels)\n",
        "        plt.xlabel(\"Similarity Score Threshold\")\n",
        "        plt.ylabel(\"False Positive Similarity Score\")\n",
        "        plt.title(\"False Positives in the Top 20 Results\")\n",
        "        plt.show()\n",
        "\n",
        "makeViolinPlot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMxXINnjOZ9"
      },
      "source": [
        "By analyzing the distribution of similarity scores and visualizing the results using histograms and violin plots, we can empirically determine an appropriate cutoff score for filtering sentences. The graphical visualizations support our choice of cutoff score. For example, using a cutoff score of .88 strikes a good balance between minimizing false positives and maximizing the number of relevant results, thereby ensuring the accuracy and effectiveness of our semantic search process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu2hAO1MlCky"
      },
      "source": [
        "# Extracting Maps from Sentences to Prevent Hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byFY3TTvlEc5"
      },
      "source": [
        "A crucial aspect of our program is the extraction of causal maps from sentences that describe cause-and-effect relationships. This step is essential in preventing \"hallucinations\" in machine learning models, which are instances where the model produces spurious or ungrounded information that is not supported by the data.\n",
        "\n",
        "To achieve this, we identify sentences that describe multiple effects of a given cause, as these sentences provide a detailed view of the causal system. We then extract the causal relationships from these sentences, creating a structured representation of the cause-and-effect links.\n",
        "\n",
        "The extracted causal maps contribute to the overall quality of the causal data used in training machine learning models. By ensuring that the data accurately reflects real-world causality and minimizing the risk of hallucinations, we enhance the interpretability and reliability of the insights derived from the models.\n",
        "\n",
        "This extraction process is a vital step towards building models that can effectively analyze and understand the complexity of real-world causal systems, leading to more accurate and actionable predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "5-hOM2X5lGK2",
        "outputId": "f05c7f8c-7b48-4269-8c29-6e9e7ef2d98d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def format_text(input_text, sentence):\n",
        "    try:\n",
        "        formatted_text = \"<S> \"\n",
        "        \n",
        "        # Split input_text into individual tuples using \"), (\" as the delimiter\n",
        "        input_tuples = input_text.strip(\"()\").split(\"), (\")\n",
        "\n",
        "        for item in input_tuples:\n",
        "            input_tuple = item.split(\", \")\n",
        "\n",
        "            correlation = input_tuple[0].strip(\"'\")\n",
        "            cause = input_tuple[1].strip(\"'\")\n",
        "            \n",
        "            # Extract all elements starting from the third element as effects and strip single quotes\n",
        "            effects = [effect.strip(\"'\") for effect in input_tuple[2:]]\n",
        "\n",
        "            correlation_tag = \"<POS>\" if correlation == '1' else \"<NEG>\"\n",
        "\n",
        "            # Iterate through each effect for a given cause\n",
        "            for effect in effects:\n",
        "                formatted_text += f\"<H> {cause} {correlation_tag} <T> {effect.strip()} \"\n",
        "\n",
        "        formatted_text += \"<E>\"\n",
        "        return formatted_text\n",
        "    except Exception as e:\n",
        "        # Print exception message and traceback\n",
        "        import traceback\n",
        "        print(str(e))\n",
        "        traceback.print_exc()\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def extract_cause_and_effect(sentence):\n",
        "  # Define the prompt\n",
        "  prompt = f'''\n",
        "  Extract the cause and effect from the following sentence: '{sentence}' and print the causes and effects as a tuple, taking into account closely related causes when identifying cause and effect relationships. If a cause leads to multiple positive effects, group them together and label the correlation as 1. If a cause leads to multiple negative effects, group them together and label the correlation as -1.\n",
        "  For example, if the sentence is 'Exposure to air pollution is associated with an increased risk of respiratory diseases, cardiovascular diseases, and premature death. However, living in areas with good air quality reduces the risk of respiratory diseases.', the output should be [('1', 'air pollution', 'respiratory diseases, cardiovascular diseases, premature death'), ('-1', 'good air quality', 'respiratory diseases')].\n",
        "  When extracting causes, consider the context and identify closely related causes. If you find causes that are similar or closely related, you must group them. The output should be a string. Each effect should be separate so that the output is a list of tuples.\n",
        "  For example, if the sentence is 'The environmental consequences of deforestation include soil erosion, loss of biodiversity, and climate change.', the output should be [('1', 'deforestation', 'soil erosion, loss of biodiversity, climate change')].\n",
        "  For example, if the sentence is 'Excessive screen time in children and adolescents can lead to various health issues, including eye strain, poor posture, and sleep disturbances. On the other hand, engaging in physical activities can improve overall health and enhance mental well-being.', the output should be [('1', 'excessive screen time', 'eye strain, poor posture, sleep disturbances'), ('-1', 'physical activities', 'mental well-being')].\n",
        "  Note that the causes and effects should be in lowercase and should not contain any punctuation. The cause and effect should be separated by a comma and a space. The cause-effect pairs should be separated by a space. The cause and the effect can not be 1 or -1.\n",
        "  '''\n",
        "\n",
        "  retries = 5\n",
        "  delay = 10\n",
        "  backoff_factor = 3\n",
        "\n",
        "  while retries > 0:\n",
        "      try:\n",
        "          response = openai.ChatCompletion.create(\n",
        "              # gpt-4\n",
        "              model=\"gpt-4\",\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": \"You are a helpful assistant that only returns a tuple\"},\n",
        "                  {\"role\": \"user\", \"content\": prompt},\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          # Extract the assistant's reply\n",
        "          assistant_reply = response['choices'][0]['message']['content']\n",
        "          return assistant_reply\n",
        "\n",
        "      except Exception as e:\n",
        "          time.sleep(delay)\n",
        "          retries -= 1\n",
        "          delay *= backoff_factor\n",
        "\n",
        "  raise Exception(\"Rate limit error: Maximum retries reached\")\n",
        "\n",
        "def process_sentences_from_results_folder(input_folder: str, output_filename: str) -> None:\n",
        "    unique_sentences = set()\n",
        "\n",
        "    with open(output_filename, 'w', newline='') as output_file:\n",
        "        csv_writer = csv.writer(output_file)\n",
        "        csv_writer.writerow(['subgraph', 'sentence'])\n",
        "\n",
        "        for file in os.listdir(input_folder):\n",
        "            if file.endswith('.csv'):\n",
        "                file_path = os.path.join(input_folder, file)\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                # Assuming one sentence per line in the 'text' column\n",
        "                for index, row in df.iterrows():\n",
        "                    sentence = row['text']\n",
        "                    \n",
        "                    # Skip the sentence if it has already been processed\n",
        "                    if sentence in unique_sentences:\n",
        "                        continue\n",
        "\n",
        "                    unique_sentences.add(sentence)\n",
        "                    cause_and_effect = extract_cause_and_effect(sentence)  # Call the extract_cause_and_effect function\n",
        "                    formatted_text = format_text(cause_and_effect, sentence)  # Apply the format_text function\n",
        "\n",
        "                    # Write the processed sentence to the output file\n",
        "                    csv_writer.writerow([formatted_text, sentence])\n",
        "\n",
        "process_sentences_from_results_folder('results', 'finalresult.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STc1n8AnMMLg"
      },
      "source": [
        "Some final data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CAjqfxKEKUSv",
        "outputId": "b0300afd-3ef0-41b7-8933-b0697515f402"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def remove_brackets_and_chars_from_csv(file_path, column_name):\n",
        "    def remove_brackets_and_chars(text):\n",
        "        text = text.replace(\"(\", \"\").replace(\")\", \"\")  # Remove round brackets\n",
        "        text = text.replace(\"[\", \"\").replace(\"]\", \"\")  # Remove square brackets\n",
        "        text = text.replace(\"'\", \"\")                   # Remove single quotes\n",
        "        text = re.sub(r'(?<![0-9A-Za-z_])-1(?![0-9A-Za-z_])', '', text)  # Remove standalone -1\n",
        "        return text\n",
        "\n",
        "    def change_pos_to_neg(text):\n",
        "        words_to_negate = ['reduce', 'decrease', 'prevent']  # Put your words here\n",
        "        for word in words_to_negate:\n",
        "            text = re.sub(rf'<POS>\\s*<T>\\s*\\b{word}\\b', '<NEG> <T>', text, count=1)\n",
        "        return text\n",
        "\n",
        "    def remove_certain_words(text):\n",
        "        words_to_remove = ['increase', 'increased', 'improve']  # Put your words here\n",
        "        for word in words_to_remove:\n",
        "            text = re.sub(rf'(<POS>\\s*<T>\\s*)\\b{word}\\b', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "    text_to_remove = 'are no specific causes and effects mentioned to be extracted'\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        if column_name not in df.columns:\n",
        "            return f\"The column '{column_name}' was not found in the CSV file.\"\n",
        "\n",
        "        df[column_name] = df[column_name].apply(remove_brackets_and_chars)\n",
        "        df[column_name] = df[column_name].apply(change_pos_to_neg)\n",
        "        df[column_name] = df[column_name].apply(remove_certain_words)\n",
        "\n",
        "        df = df[~df[column_name].str.contains(r'\\<T\\>\\s+\\<E\\>', na=False, regex=True)]\n",
        "        \n",
        "        df = df[~df.applymap(lambda x: text_to_remove in str(x)).any(axis=1)]\n",
        "\n",
        "        df.to_csv(file_path, index=False)\n",
        "\n",
        "        return f\"Successfully modified and saved changes to '{file_path}'.\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"The file '{file_path}' was not found.\"\n",
        "\n",
        "# Example usage:\n",
        "remove_brackets_and_chars_from_csv('finalresult.csv', 'subgraph')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy5PMjnT10Lp"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHjDsY5x9LIZ"
      },
      "source": [
        "In conclusion, the program we have developed effectively performs semantic search and causal map extraction from PDF documents to enhance our understanding of causal relationships within the text. Through a series of carefully orchestrated steps, including preprocessing of PDF documents, embedding and similarity search, as well as extracting detailed causal maps, we have successfully identified sentences that align with the causal relationships depicted in the maps. Notably, the program's ability to extract maps from sentences describing multiple effects helps mitigate the risk of hallucinations in machine learning models, thereby ensuring that the models produce reliable and interpretable insights.\n",
        "\n",
        "As a result, this program serves as a powerful tool for researchers, data scientists, and domain experts seeking to extract valuable causal information from textual documents. By automating the extraction process, we save time and resources while maintaining a high level of accuracy. The program's potential to contribute to the advancement of machine learning models in understanding complex causal systems demonstrates its value and importance in the field of natural language processing and causal inference."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
